{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc416a02-869b-4c6c-8230-fa42e72e66ce",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## User-defined functions (UDFs) and vectorized UDFs\n",
    "\n",
    "In Spark, a User-Defined Function (UDF) is a feature that allows you to define your own transformations using Python or Scala. UDFs work by transforming values from a single row within the dataframe. However, they have a performance overhead because each function needs a Python API call instead of running pure Spark code.\n",
    "\n",
    "Vectorized UDFs, on the other hand, are introduced in Spark to make Python UDFs more efficient. With vectorized UDFs, you can directly apply a function to whole columns in the dataframe, making use of vectorized operations provided by libraries like Pandas. This reduces the overhead and improves performance.\n",
    "\n",
    "Let's see an example of both UDF and vectorized UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf23bf6-d92e-4b7d-a8e1-e37ca79483f0",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n|value|value_plus_one|\n+-----+--------------+\n|    1|             2|\n|    2|             3|\n|    3|             4|\n+-----+--------------+\n\n+-----+-------------------------+\n|value|value_plus_one_vectorized|\n+-----+-------------------------+\n|    1|                        2|\n|    2|                        3|\n|    3|                        4|\n+-----+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Creating a Spark session\n",
    "spark = SparkSession.builder.appName('UDF_example').getOrCreate()\n",
    "\n",
    "# Sample dataframe\n",
    "df = spark.createDataFrame([(1,), (2,), (3,)], ['value'])\n",
    "\n",
    "# Regular UDF\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "add_one_udf = udf(add_one, IntegerType())\n",
    "df.withColumn('value_plus_one', add_one_udf(df['value'])).show()\n",
    "\n",
    "# Vectorized UDF\n",
    "@pandas_udf('int')\n",
    "def add_one_vectorized(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "df.withColumn('value_plus_one_vectorized', add_one_vectorized(df['value'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6584975c-424c-48cf-8865-6d83597b7a0a",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Spark Internals\n",
    "\n",
    "Apache Spark is a distributed computing system that processes large datasets in parallel. Understanding its internals helps in optimizing Spark applications for better performance. Here are some key components:\n",
    "\n",
    "- **Driver Program**: It's the central point and the entry point of the Spark Application. It runs the main function and creates the SparkContext.\n",
    "- **SparkContext**: It's the client-side object that represents the connection to a Spark cluster and is used to create RDDs, accumulators, and broadcast variables on that cluster.\n",
    "- **Cluster Manager**: Spark supports different cluster managers like Standalone, Mesos, and YARN. It's responsible for acquiring resources on the Spark cluster.\n",
    "- **Executor**: Once the SparkContext connects to the Cluster Manager, it acquires executors on nodes in the cluster. Executors are Spark processes that run computations and store data for your application.\n",
    "- **Task**: It's a unit of work that can be run on a partition of a distributed dataset and gets executed on a single executor.\n",
    "- **Job**: A parallel computation consisting of multiple tasks that get spawned in response to a Spark action (e.g., save, collect).\n",
    "- **Stage**: Jobs are divided into stages. Stages are classified as a set of tasks based on their dependencies on each other.\n",
    "\n",
    "Let's visualize the Spark internals with a flowchart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102ecadf-6ac5-44b2-8ac2-854cd8831445",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Query Optimization\n",
    "\n",
    "Query optimization is a crucial aspect of Spark that ensures efficient execution of operations. Spark's Catalyst engine handles query optimization. Here's how it works:\n",
    "\n",
    "- **Logical Optimization**: This step involves rule-based transformations on the logical plan to optimize it. Examples include predicate pushdown, constant folding, and boolean simplifications.\n",
    "- **Physical Planning**: The logical plan is translated into one or more physical plans. The best plan is selected based on cost estimation.\n",
    "- **Code Generation**: To improve performance, Spark generates bytecode for the selected physical plan, which is then executed on the JVM.\n",
    "\n",
    "Spark's ability to optimize queries means that developers can focus on the logic of their applications without worrying too much about performance tuning.\n",
    "\n",
    "Let's see an example that demonstrates query optimization in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b0706e-3818-42fb-9b3c-172d8abb8e94",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Project [Name#32, Age#33L, '`+`('Age, 5) AS Age after 5 years#36]\n+- Filter (Age#33L > cast(25 as bigint))\n   +- LogicalRDD [Name#32, Age#33L], false\n\n== Analyzed Logical Plan ==\nName: string, Age: bigint, Age after 5 years: bigint\nProject [Name#32, Age#33L, (Age#33L + cast(5 as bigint)) AS Age after 5 years#36L]\n+- Filter (Age#33L > cast(25 as bigint))\n   +- LogicalRDD [Name#32, Age#33L], false\n\n== Optimized Logical Plan ==\nProject [Name#32, Age#33L, (Age#33L + 5) AS Age after 5 years#36L]\n+- Filter (isnotnull(Age#33L) AND (Age#33L > 25))\n   +- LogicalRDD [Name#32, Age#33L], false\n\n== Physical Plan ==\n*(1) Project [Name#32, Age#33L, (Age#33L + 5) AS Age after 5 years#36L]\n+- *(1) Filter (isnotnull(Age#33L) AND (Age#33L > 25))\n   +- *(1) Scan ExistingRDD[Name#32,Age#33L]\n\n== Photon Explanation ==\nPhoton does not fully support the query because:\n\t\tUnsupported node: Scan ExistingRDD[Name#32,Age#33L].\n\nReference node:\n\tScan ExistingRDD[Name#32,Age#33L]\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample dataframe\n",
    "data = [('Alice', 25), ('Bob', 30), ('Catherine', 29)]\n",
    "columns = ['Name', 'Age']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Applying multiple transformations\n",
    "df_transformed = df.filter(F.col('Age') > 25).withColumn('Age after 5 years', F.col('Age') + 5)\n",
    "\n",
    "# Show the execution plan\n",
    "df_transformed.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06604b8d-6edc-4ef9-9884-98a176fba8db",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Partitioning\n",
    "\n",
    "Partitioning is a fundamental concept in Spark that allows it to distribute data across the cluster and parallelize operations. Here's a brief overview:\n",
    "\n",
    "- **What is Partitioning?**\n",
    "  - Partitioning is the process of dividing a large dataset into smaller chunks (partitions) that can be processed in parallel.\n",
    "\n",
    "- **Why is it Important?**\n",
    "  - Partitioning ensures that data related to a particular computation is located close to the computation, reducing data shuffling and improving performance.\n",
    "  - It allows Spark to parallelize operations, leading to faster execution.\n",
    "\n",
    "- **Types of Partitioning:**\n",
    "  - **Hash Partitioning**: Data is partitioned based on the hash value of the partition key.\n",
    "  - **Range Partitioning**: Data is partitioned based on a range of values of the partition key.\n",
    "\n",
    "- **Managing Partitions:**\n",
    "  - Spark provides methods like `repartition()` and `coalesce()` to increase or decrease the number of partitions.\n",
    "\n",
    "Let's see an example demonstrating partitioning in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8976f174-eb1d-4b40-965e-8c860dbca20b",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 3, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of partitions in the original dataframe\n",
    "num_partitions = df.rdd.getNumPartitions()\n",
    "\n",
    "# Repartitioning the dataframe into 3 partitions\n",
    "df_repartitioned = df.repartition(3)\n",
    "num_partitions_repartitioned = df_repartitioned.rdd.getNumPartitions()\n",
    "\n",
    "# Reducing the number of partitions using coalesce\n",
    "df_coalesced = df_repartitioned.coalesce(2)\n",
    "num_partitions_coalesced = df_coalesced.rdd.getNumPartitions()\n",
    "\n",
    "num_partitions, num_partitions_repartitioned, num_partitions_coalesced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f98e34-54e7-4507-bc7e-93fe5c060e4c",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Streaming API\n",
    "\n",
    "Spark's Streaming API allows for processing real-time data streams. It provides a high-level API for stream processing using the same batch processing model that Spark's core API uses. Here are some key points:\n",
    "\n",
    "- **DStreams**: At the heart of Spark Streaming is the concept of Discretized Stream or DStream, which represents a continuous stream of data. DStreams can be created by ingesting data from various sources like Kafka, Flume, and HDFS.\n",
    "- **Micro-batching**: Spark Streaming processes data using a micro-batching approach. The incoming data is divided into small batches, and these batches are processed using Spark's core engine.\n",
    "- **Stateful Operations**: Spark Streaming provides built-in functions to maintain and query state information over time, such as `updateStateByKey` and `window` operations.\n",
    "- **Integration with Other Spark Components**: Spark Streaming can be easily integrated with other Spark components like MLlib and GraphX for machine learning and graph processing on streaming data.\n",
    "\n",
    "Let's see a simple example of Spark Streaming (Note: This is a conceptual example and won't produce real-time streaming output in this notebook environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a974df9-ab2d-4c35-8b6e-02fbfe3e7071",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Creating a local StreamingContext with two execution threads and a batch interval of 1 second\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "# Creating a DStream that reads data from localhost:9999\n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Splitting each line into words\n",
    "words = lines.flatMap(lambda line: line.split(' '))\n",
    "\n",
    "# Counting each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Printing the first ten elements of each RDD generated in this DStream\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Note: To run this code, you would need to first run a Netcat server using 'nc -lk 9999' in a terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb04b513-0ff8-4253-a322-24ea6e8035af",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Delta Lake\n",
    "\n",
    "Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It's designed to provide reliability, performance, and data integrity to data lakes. Here are some key features and benefits of Delta Lake:\n",
    "\n",
    "- **ACID Transactions**: Delta Lake ensures that data is stored reliably in the data lake by providing full ACID transaction capabilities.\n",
    "- **Scalable Metadata Handling**: It can handle petabyte-scale tables with billions of partitions and files.\n",
    "- **Time Travel (Data Versioning)**: Delta Lake provides snapshots of data, enabling developers to access and revert to earlier versions of data for audits, rollbacks, or reproducing experiments.\n",
    "- **Unified Batch and Streaming Source and Sink**: With Delta Lake, you can run batch and streaming workloads simultaneously on the same dataset.\n",
    "- **Schema Enforcement and Evolution**: It ensures data integrity by enforcing schemas and allowing for schema evolution.\n",
    "\n",
    "Delta Lake works over your existing data and is fully compatible with the Spark API. This means you can use Delta Lake with your existing Spark code.\n",
    "\n",
    "Let's see a simple example of using Delta Lake (Note: This is a conceptual example and might require additional setup in a real environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b1c407-643f-47bd-a989-f8c4da4e62e5",
     "showTitle": false,
     "title": ""
    },
    "noteable": {
     "cell_type": "code"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n|first_name|last_name|age|\n+----------+---------+---+\n|      John|      Doe| 29|\n|      Jane|      Doe| 25|\n+----------+---------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries for Delta Lake\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Creating a sample dataframe\n",
    "data = [('John', 'Doe', 29), ('Jane', 'Doe', 25)]\n",
    "columns = ['first_name', 'last_name', 'age']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Writing the dataframe to Delta Lake format\n",
    "path = '/tmp/delta-table'\n",
    "df.write.format('delta').save(path)\n",
    "\n",
    "# Reading from Delta Lake\n",
    "df_delta = spark.read.format('delta').load(path)\n",
    "df_delta.show()\n",
    "\n",
    "# Note: This is a conceptual example. In a real environment, you might need to set up Delta Lake and its dependencies."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_Details_and_Examples",
   "widgets": {}
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "noteable": {
   "last_transaction_id": "06e13389-be01-4625-8f4e-ae6808cd1c35"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "3a68c733-aa4e-5136-8ae8-b0dc6707cbd5",
    "openai_ephemeral_user_id": "9fc85f72-3f60-5f38-b137-15ea5af26c0c",
    "openai_subdivision1_iso_code": "IN-CH"
   }
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
